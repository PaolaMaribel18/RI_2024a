{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaolaMaribel18/RI_2024a/blob/main/week12/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMcU6OKFpL72",
        "outputId": "6c22a6fb-5a45-4237-bcdb-7d059a931637"
      },
      "id": "EMcU6OKFpL72",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "88b63181c563a666"
      },
      "cell_type": "markdown",
      "source": [
        "# Web Scraping Exercise\n",
        "\n",
        "## 1. Introduction and Planning\n",
        "\n",
        "### Objective:\n",
        "The goal of this exercise is to build a web scraper that collects data from a chosen website. You will learn how to send HTTP requests, parse HTML content, extract relevant data, and store it in a structured format.\n",
        "\n",
        "### Tasks:\n",
        "1. Identify the data you want to scrape.\n",
        "2. Choose the target website(s).\n",
        "3. Plan the structure of your project.\n",
        "\n",
        "### Example:\n",
        "For this exercise, we will scrape job listings from Indeed.com. We will extract job titles, company names, locations, and job descriptions."
      ],
      "id": "88b63181c563a666"
    },
    {
      "metadata": {
        "id": "477cce632174e459"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Understanding the Target Website\n",
        "### Objective:\n",
        "\n",
        "Analyze the structure of the web pages to be scraped.\n",
        "### Tasks:\n",
        "\n",
        "* Inspect the target website using browser developer tools.\n",
        "* Identify the HTML elements that contain the desired data.\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "* Open your browser and navigate to the target website (e.g., Indeed.com).\n",
        "* Right-click on the webpage and select \"Inspect\" or press Ctrl+Shift+I.\n",
        "* Use the developer tools to explore the HTML structure of the webpage.\n",
        "* Identify the tags and classes of the elements that contain the job titles, company names, locations, and descriptions."
      ],
      "id": "477cce632174e459"
    },
    {
      "metadata": {
        "id": "393d4bc45393e6b5"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Writing the Scraper\n",
        "### Objective:\n",
        "\n",
        "Develop the code to scrape data from the target website.\n",
        "### Tasks:\n",
        "\n",
        "* Send HTTP requests to the target website.\n",
        "* Parse the HTML content and extract the required data.\n",
        "* Handle pagination to scrape data from multiple pages.\n",
        "* Implement error handling."
      ],
      "id": "393d4bc45393e6b5"
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "y2M4XdsvrSEN"
      },
      "id": "y2M4XdsvrSEN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HTTP headers configuration\n",
        "headers = {\n",
        "    \"User-Agent\": \"My Web App\"\n",
        "}"
      ],
      "metadata": {
        "id": "QrPc4fUYrTy_"
      },
      "id": "QrPc4fUYrTy_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read HTML content from a local file and parse it with BeautifulSoup\n",
        "def read_html_file(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        return file.read()"
      ],
      "metadata": {
        "id": "2XVvE5irrWng"
      },
      "id": "2XVvE5irrWng",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_content = read_html_file(\"/content/drive/MyDrive/week12/allRecipes.html\")\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")"
      ],
      "metadata": {
        "id": "3EBXX-qgralH"
      },
      "id": "3EBXX-qgralH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch HTML content from a list of URLs\n",
        "def fetch_html_from_urls(urls_list, headers, timeout=10):\n",
        "    html_contents = []\n",
        "    for url in tqdm(urls_list, desc=\"Downloading\"):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=timeout)\n",
        "            content = BeautifulSoup(response.text, \"html.parser\")\n",
        "            html_contents.append(content)\n",
        "        except requests.Timeout:\n",
        "            print(f\"Timeout occurred for URL: {url}\")\n",
        "        except requests.RequestException as error:\n",
        "            print(f\"Request failed for URL: {url}. Error: {error}\")\n",
        "    return html_contents"
      ],
      "metadata": {
        "id": "-fCPEdrxrdf7"
      },
      "id": "-fCPEdrxrdf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract links from an HTML page\n",
        "def extract_links_from_page(html_elements):\n",
        "    links = []\n",
        "    for item in html_elements.find_all(\"li\", class_=\"comp mntl-link-list__item\"):\n",
        "        anchor = item.find(\"a\", href=True)\n",
        "        if anchor:\n",
        "            links.append(anchor['href'])\n",
        "    return links"
      ],
      "metadata": {
        "id": "uBS78urVre44"
      },
      "id": "uBS78urVre44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract links from a specific page using CSS classes\n",
        "def extract_links_from_list(page_elements):\n",
        "    links = []\n",
        "    for item in page_elements.find_all(\"a\", class_=\"comp mntl-card-list-items mntl-document-card mntl-card card card--no-image\"):\n",
        "        if item.has_attr('href'):\n",
        "            links.append(item['href'])\n",
        "    return links"
      ],
      "metadata": {
        "id": "SGhR3FFbriNG"
      },
      "id": "SGhR3FFbriNG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get titles from a list of BeautifulSoup objects\n",
        "def get_titles_from_pages(pages):\n",
        "    titles = [page.find(\"title\").text for page in pages]\n",
        "    return titles\n"
      ],
      "metadata": {
        "id": "E7hmYKyprlhy"
      },
      "id": "E7hmYKyprlhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recipe descriptions\n",
        "def get_recipe_descriptions(pages_recipes):\n",
        "    descriptions = []\n",
        "    for page in pages_recipes:\n",
        "        description = page.find(\"p\", class_=\"article-subheading type--dog\")\n",
        "        descriptions.append(description.text if description else np.nan)\n",
        "    return descriptions"
      ],
      "metadata": {
        "id": "JPzLR8y2ro21"
      },
      "id": "JPzLR8y2ro21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recipe ingredients\n",
        "def get_recipe_ingredients(pages_recipes):\n",
        "    ingredients = []\n",
        "    for page in pages_recipes:\n",
        "        ingredient_text = ''\n",
        "        for item in page.find_all(\"li\", class_=\"mm-recipes-structured-ingredients__list-item\"):\n",
        "            ingredient_text += item.text.strip() + '\\n'\n",
        "        ingredients.append(ingredient_text)\n",
        "    return ingredients"
      ],
      "metadata": {
        "id": "I7xFi7Hmru7s"
      },
      "id": "I7xFi7Hmru7s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recipe steps\n",
        "def get_recipe_steps(pages_recipes):\n",
        "    steps = []\n",
        "    for page in pages_recipes:\n",
        "        step_text = ''\n",
        "        for item in page.find_all(\"li\", class_=\"comp mntl-sc-block mntl-sc-block-startgroup mntl-sc-block-group--LI\"):\n",
        "            for tag in item.find_all([\"figure\", \"div\"]):\n",
        "                tag.decompose()\n",
        "            step_text += item.text.strip() + '\\n'\n",
        "        steps.append(step_text)\n",
        "    return steps\n"
      ],
      "metadata": {
        "id": "ZCfFMtzYryYW"
      },
      "id": "ZCfFMtzYryYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract links from the soup HTML\n",
        "html_links = extract_links_from_page(soup)\n",
        "\n",
        "# Download HTML content from each link\n",
        "recipe_contents = fetch_html_from_urls(html_links, headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FiHf9Y01OVL",
        "outputId": "9394b221-26ca-4d4d-f581-04f986b84141"
      },
      "id": "4FiHf9Y01OVL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 378/378 [03:29<00:00,  1.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract links from each recipe page\n",
        "recipe_links = [extract_links_from_list(page) for page in recipe_contents]\n",
        "# Combine all recipe page links into a single list\n",
        "all_links = [link for sublist in recipe_links for link in sublist]\n"
      ],
      "metadata": {
        "id": "vNIgcRRG1UDZ"
      },
      "id": "vNIgcRRG1UDZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download HTML content from each link\n",
        "full_recipe_contents = fetch_html_from_urls(all_links, headers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsxZaaqWr1GH",
        "outputId": "adc638d2-d4d6-4c32-ce05-94f508204953"
      },
      "id": "AsxZaaqWr1GH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading:  17%|█▋        | 2999/18122 [23:05<1:41:08,  2.49it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recipe titles\n",
        "recipe_titles = get_titles_from_pages(full_recipe_contents)\n",
        "recipes_df = pd.DataFrame(recipe_titles, columns=['Title'])\n",
        "\n",
        "# Get descriptions and add them to the DataFrame\n",
        "recipe_descriptions = get_recipe_descriptions(full_recipe_contents)\n",
        "recipes_df['Description'] = recipe_descriptions\n",
        "\n",
        "# Get ingredients and add them to the DataFrame\n",
        "recipe_ingredients = get_recipe_ingredients(full_recipe_contents)\n",
        "recipes_df['Ingredients'] = recipe_ingredients\n",
        "\n",
        "# Get steps and add them to the DataFrame\n",
        "recipe_steps = get_recipe_steps(full_recipe_contents)\n",
        "recipes_df['Steps'] = recipe_steps\n"
      ],
      "metadata": {
        "id": "Z8bBdVousB0n"
      },
      "id": "Z8bBdVousB0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file\n",
        "recipes_df.to_csv('Complete_Recipes.csv', index=False)\n",
        "\n",
        "# Read the CSV file to verify its contents\n",
        "df = pd.read_csv('Complete_Recipes.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "J49CJowesFdl"
      },
      "id": "J49CJowesFdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "r2J6Y1A0sMke"
      },
      "id": "r2J6Y1A0sMke",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}